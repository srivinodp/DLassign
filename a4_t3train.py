# -*- coding: utf-8 -*-
"""A4_T3train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1e0mK66gNIyKRbwCJOAVLziU9QwvOE-Yw
"""

import tensorflow as tf
import numpy as np
from keras.preprocessing.text import Tokenizer
from sklearn.model_selection import train_test_split

import unicodedata
import io
import time

from google.colab import drive
drive.mount('/content/drive')

embeddings_index = dict()
f = open('/content/drive/My Drive/glove.6B.300d.txt',encoding="utf-8")
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()
print('Loaded %s word vectors.' % len(embeddings_index))

docs=embeddings_index.keys()
tokenizer_en = Tokenizer(num_words=400000)
tokenizer_en.fit_on_texts(docs)
vocab_size = len(tokenizer_en.word_index) + 3
embedding_matrix = np.zeros((vocab_size, 300))
for word, i in tokenizer_en.word_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector

train_dataset_en=tf.data.TextLineDataset('/content/drive/My Drive/train.en')
train_dataset_ta=tf.data.TextLineDataset('/content/drive/My Drive/train.ta')
train_dataset_en=[str(i.decode('utf-8')).replace('\'',' \'') for i in train_dataset_en.as_numpy_iterator()]
train_dataset_en=tf.data.Dataset.from_tensor_slices(train_dataset_en)

# print(train_dataset_en.shape)

tokenizer_ta=tf.keras.preprocessing.text.Tokenizer(num_words=400000,oov_token='oov')
texts=[str(i.decode('utf-8'))for i in train_dataset_ta.as_numpy_iterator()]

tokenizer_ta.fit_on_texts(texts)

vocab_size = len(tokenizer_en.word_index) + 3
vocab_tar_size = len(tokenizer_ta.word_index) + 3

train_dataset = tf.data.Dataset.zip((train_dataset_en, train_dataset_ta))
print(train_dataset)

train_sentences = 0
for (batch,(inp,tar)) in enumerate(train_dataset):
    #print(inp)
    #break
    train_sentences += 1
print(train_sentences)

BATCH_SIZE = 64

def convertToSequence(lang1, lang2):
    sequence1 = tokenizer_en.texts_to_sequences([lang1.numpy().decode('utf-8').lower()])
    flat_list = [item for sublist in sequence1 for item in sublist]

    lang1 = [len(tokenizer_en.word_index)+1] + flat_list + [len(tokenizer_en.word_index)+2]
    
    sequence2=tokenizer_ta.texts_to_sequences([lang2.numpy().decode('utf-8')])
    
    flat_list = [item for sublist in sequence2 for item in sublist]

    lang2 = [len(tokenizer_ta.word_index)+1] + flat_list + [len(tokenizer_ta.word_index)+2]


    # lang2 = [tokenizer_ta.vocab_size+1] + tokenizer_ta.encode(lang2.numpy()) + [tokenizer_ta.vocab_size+2]

    return lang1, lang2

def toSequence(en, ta):
    result_en, result_ta = tf.py_function(convertToSequence, [en, ta], [tf.int64, tf.int64])
    result_en.set_shape([None])
    result_ta.set_shape([None])

    return result_en, result_ta

train_dataset_indices = train_dataset.map(toSequence)
padded_train_dataset = train_dataset_indices.padded_batch(BATCH_SIZE,padded_shapes=([None],[None]))
for (batch,(inp,tar)) in enumerate(padded_train_dataset):
    print(inp)
    print(tar)
    break
# print(len(input_tensor),len(input_tensor))

class Encoder(tf.keras.Model):
  def __init__(self, vocab_size, embedding_matrix, enc_units, siz_batch):
    super(Encoder, self).__init__()
    self.batch_sz = siz_batch
    self.enc_units = enc_units
    self.embedding = tf.keras.layers.Embedding(vocab_size, 300,embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),trainable=False)
    self.lstm = tf.keras.layers.LSTM(self.enc_units,
                                   return_sequences=True,
                                   return_state=True,
                                   recurrent_initializer='glorot_uniform')

  def call(self, x, hidden):
    x = self.embedding(x)
    output, state, cell = self.lstm(x, initial_state = hidden)
    return output, state, cell

  def initialize_hidden_state(self):
    return tf.zeros((self.batch_sz, self.enc_units))

units = 1024
BATCH_SIZE = 64
encoder = Encoder(vocab_size, embedding_matrix, units, BATCH_SIZE)
print(encoder.trainable_variables)

sample_hidden = [encoder.initialize_hidden_state(),encoder.initialize_hidden_state()]
for (batch,(inp,tar)) in enumerate(padded_train_dataset):
    sample_output, sample_hidden, sample_cell = encoder(inp, sample_hidden)
    print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))
    print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))
    break
    # print(t.shape)
# print(batch)

class Decoder(tf.keras.Model):
  def __init__(self, vocab_size, embedding_dim, dec_units, siz_batch):
    super(Decoder, self).__init__()
    self.batch_sz = siz_batch
    self.dec_units = dec_units
    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)
    self.lstm = tf.keras.layers.LSTM(self.dec_units,
                                   return_sequences=True,
                                   return_state=True,
                                   recurrent_initializer='glorot_uniform')
    self.fc = tf.keras.layers.Dense(vocab_size)

   
  def call(self, x, hidden):
    
    x = self.embedding(x)

    output, state, cell = self.lstm(x,initial_state = hidden)

    output = tf.reshape(output, (-1, output.shape[2]))

    x = self.fc(output)

    return x, state, cell

embedding_dim = 256
decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)
sample_hidden = [encoder.initialize_hidden_state(),encoder.initialize_hidden_state()]

sample_decoder_output, sample_state, sample_cell = decoder(tf.random.uniform((BATCH_SIZE, 1)),
                                      sample_hidden)

print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))

optimizer = tf.keras.optimizers.Adam()
loss_object = tf.keras.losses.SparseCategoricalCrossentropy(
    from_logits=True, reduction='none')

def loss_function(real, pred):
  mask = tf.math.logical_not(tf.math.equal(real, 0))
  loss_ = loss_object(real, pred)

  mask = tf.cast(mask, dtype=loss_.dtype)
  loss_ *= mask

  return tf.reduce_mean(loss_)
train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')

def train_step(inp, targ, enc_hidden):
  loss = 0

  with tf.GradientTape() as tape:
    enc_output, enc_hidden, enc_cell = encoder(inp, enc_hidden)

    dec_hidden = enc_hidden
    
    dec_cell=enc_cell

    dec_input = tf.expand_dims([vocab_tar_size-2] * BATCH_SIZE, 1)

    for t in range(1, targ.shape[1]):
      predictions, dec_hidden, dec_cell = decoder(dec_input, [dec_hidden,dec_cell])

      loss += loss_function(targ[:, t], predictions)

      dec_input = tf.expand_dims(targ[:, t], 1)

  loss_for_batch = (loss / int(targ.shape[1]))
  train_accuracy(targ[:, t], predictions)

  var = encoder.trainable_variables + decoder.trainable_variables

  gradients = tape.gradient(loss, var)

  optimizer.apply_gradients(zip(gradients, var))

  return loss_for_batch

EPOCHS = 50
steps_per_epoch = train_sentences//BATCH_SIZE

for epoch in range(EPOCHS):
  start = time.time()

  enc_hidden = [encoder.initialize_hidden_state(),encoder.initialize_hidden_state()]
  total_loss = 0

  for (batch, (inp, targ)) in enumerate(padded_train_dataset.take(steps_per_epoch)):
    batch_loss = train_step(inp, targ, enc_hidden)
    total_loss += batch_loss

    if batch % 100 == 0:
      print('Epoch {} Batch {} Loss {:.4f} Accuracy {:.3f}'.format(epoch + 1,
                                                   batch,
                                                   batch_loss.numpy(),train_accuracy.result()))
 
  print('Epoch {} Loss {:.4f}'.format(epoch + 1,
                                      total_loss / steps_per_epoch))
  print('Time taken for 1 epoch {} sec\n'.format(time.time() - start))

encoder.save_weights('/content/drive/My Drive/encoder_new_lstm.h5')

decoder.save_weights('/content/drive/My Drive/decoder_new_lstm.h5')

encoder.load_weights('/content/drive/My Drive/encoder_new_lstm.h5')
decoder.load_weights('/content/drive/My Drive/decoder_new_lstm.h5')

def evaluate(sentence,max_length_targ=100):

  start_token = [len(tokenizer_en.word_index)+1]
  end_token = [len(tokenizer_en.word_index) + 2]
  
  st=tokenizer_en.texts_to_sequences([sentence.lower()])
  flat_list = [item for sublist in st for item in sublist]
  inp_sentence = start_token + flat_list + end_token
  encoder_input = tf.expand_dims(inp_sentence, 0)
  
  result = []

  hidden = [tf.zeros((1, units)),tf.zeros((1, units))]
  enc_out, enc_hidden, enc_cell = encoder(encoder_input, hidden)

  dec_hidden = enc_hidden

  dec_cell = enc_cell

  dec_input = tf.expand_dims([vocab_tar_size-2] , 0)

  for t in range(max_length_targ):
    predictions, dec_hidden, dec_cell = decoder(dec_input,[dec_hidden,dec_cell])

    
    predicted_id = tf.argmax(predictions[0]).numpy()

    
    result += [predicted_id]
    
    if predicted_id == vocab_tar_size-1:
      return result, sentence
    
    
    dec_input = tf.expand_dims([predicted_id], 0)

  return result, sentence

def translate(sentence, plot=''):
  result,sentence = evaluate(sentence.replace('\'',' \''))
  print(result)
  predicted_sentence = tokenizer_ta.sequences_to_texts([result[:-1]])  
  return predicted_sentence[0]

print(translate("They're terraforming."))
print ("நான் ஞாயிறுகளில் அவளை நாம் மற்ற கூறினார்.")

encoder.load_weights('/content/drive/My Drive/encoder_new_lstm.h5')

decoder.load_weights('/content/drive/My Drive/decoder_new_lstm.h5')

test_dataset_en=tf.data.TextLineDataset('/content/drive/My Drive/test.en')
test_dataset_ta=tf.data.TextLineDataset('/content/drive/My Drive/test.ta')

test_dataset_en=[str(i.decode('utf-8')).replace('\'',' \'') for i in test_dataset_en.as_numpy_iterator()]
test_dataset_en=tf.data.Dataset.from_tensor_slices(test_dataset_en)

test_dataset = tf.data.Dataset.zip((test_dataset_en, test_dataset_ta))

from nltk.translate.bleu_score import sentence_bleu
def bleu_score_function(reference,candidate):
  bleu1=sentence_bleu([reference], candidate,weights=(1, 0, 0, 0))
  bleu2=sentence_bleu([reference], candidate,weights=(0.5, 0.5, 0, 0))
  bleu3=sentence_bleu([reference], candidate,weights=(0.33, 0.33, 0.33, 0))
  bleu4=sentence_bleu([reference], candidate,weights=(0.25, 0.25, 0.25, 0.25))
  return bleu1,bleu2,bleu3,bleu4

import string

scores1=[]
scores2=[]
scores3=[]
scores4=[]
for(inp,tar) in test_dataset:
  prediction=translate(inp.numpy().decode('utf-8').lower().replace('\'',' \''))
  print(inp.numpy())
  reference=tar.numpy().decode('utf-8').split(' ')
  reference=[''.join(c for c in s if c not in string.punctuation) for s in reference]
  candidate=prediction.split(' ')
  print(reference)
  print(candidate)
  bleu1,bleu2,bleu3,bleu4 = bleu_score_function(reference, candidate)
  print(bleu1)
  print(bleu2)
  print(bleu3)
  print(bleu4)
  scores1.append(bleu1)
  scores2.append(bleu2)
  scores3.append(bleu3)
  scores4.append(bleu4)


print(sum(scores1)/len(scores1))
print(sum(scores2)/len(scores2))
print(sum(scores3)/len(scores3))
print(sum(scores4)/len(scores4))

pip install nltk --upgrade